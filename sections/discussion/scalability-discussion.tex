\section{Scalability in the system}
\paragraph{Scalability principles}
In \autoref{sec:scalability} we determined that there are different types of scalability, and that when designing for scalability certain principles should be followed.
In this section we will examine and discuss which of these principles we followed throughout the creation of the system.

\begin{itemize}
    \item \textbf{Design for two for everything}: This is a principle we considering for designing the interface of the system as well as getting data for users.
    For front end pages that show a variable amount of items we considered how to accommodate this in the design to ensure it would decent.
    For getting data to the front end it was also a consideration, but not every part of the system deals wit hit optimally.
    The show all services page, for example, will receive and attempt to show all services, which could pose problems in terms of scalability.
    A way to mitigate this would be through introducing pagination to this page.
    \item \textbf{Scale horizontally}. This was a concern throughout development, and as can be seen \todo{Laver vi noget testing på det, i så fald opdater lige det her lort}
    \item \textbf{Build API first}. Whenever we wanted to implement a new part of the system, one of the first and primary concerns was how to accommodate this feature in the API.
    The reason for this was to make it could function as a service usable by any system rather than tailored to the frontend. 
    \item \textbf{Caching data}. Caching data after retrieval could help increase performance of the system if a user were to quickly swap between separate pages. 
    As it stands, this system does not take advantage of caching.
    \item \textbf{Eventual consistency}. The system does not have any parts that strictly require consistency, meaning scalability through availability should be possible. 
    If a user is browsing the services, it is not imperative that, if a new service were to be added, it would update the page the user was using.
    It would simply update the next time the user performed an action to change the state of the system, at which point it would be reloaded and shown.
    \item \textbf{Easy maintenance and automation}. In order to make maintenance easy, the system has been designed to split responsibility.
    If an update to the system was necessary, we attempted to make it easy by ensuring it would not require changes in multiple files.
    \item \textbf{Write asynchronous code}. Allowing tasks to be executed on different threads is a way to increase availability. 
    This was a primary concern when implementing the system, and all interactions with the database, and interactions between the front end and API are processed asynchronously.
    \item \textbf{Storing information in the state}. The system does not store information about a component's state in the system servers.
    As part of using React, the system stores information about states on the client side.
    This means that any server can handle any client request, and they will not depend on each other.
\end{itemize}
We attempted to keep many of these design principles in mind when creating the system to increase scalability.
However, we did not fully manage to account for all of them.
A principle such as caching data would be beneficial, and is an obvious point of improvement for the system.
Another point of improvement could be introducing pagination on some of the components that would benefit from it, to ensure the amount of data does not scale to be unnecessarily large.
In terms of the different types of scalability also defined in \autoref{sec:scalability}, we focused on performance, maintenance and availability.
The above issues relate to scalability in performance, meaning this type of scalability could be improved.

\paragraph{Deployment architecture}
If the system were to be deployed, the diagram of components would be as illustrated in \autoref{fig:deployment-diagram}.
\begin{figure}[H]
    \includegraphics[height=15cm]{/deployment.JPG}
     \caption{The deployment diagram for the system}
     \label{fig:deployment-diagram}
 \end{figure}
 \noindent
As shown, this would involve a horizontal scaling, with the main changes happening in the load balancer and server layer.
The load balancer would be responsible for distributing clients to different API clusters.
While there are different API cluster in order to spread out client requests, each cluster would also have multiple instances of the API, meaning an API cluster could consists of one to many API's.

\subsection{Evaluation of API scalability}
To evaluate the scalability in terms of availability, we ran load tests to test whether using a vertical scaling model worked.
As most of the business logic is handled by the API, it was decided to focus load test on the API.
We decided to do a peak load test using the tool LoadUI, which allows us to define an amount of arriving virtual users (VUs). %https://support.loadimpact.com/3.0/test-configuration/what-are-virtual-users-vus/
These VUs work as concurrent users that can each open multiple connections in parallel to simulate having a large amount of users constantly arriving to the site.
Unlike real users, these VUs will continuously send requests to the server at a rate of approximately 18 requests per second, as seen on the results in %ref appendix.

For the load tests we used a setup where 20 new VUs would arrive every 10th second, and a test duration of five minutes.
The tests were ran with the following vertical scaling setups:
\begin{itemize}
    \item 1 cluster with 1 instance
    \item 1 cluster with 1 instance
    \item 1 cluster with 8 instances
    \item 1 cluster with 16 instances
    \item 1 cluster with 32 instances
\end{itemize}
For all the tests, the API clusters were run on a Lenovo Legion Y530 laptop, using PM2 for handling the vertical scaling.

\begin{table}[]
    \begin{tabular}{|l|l|l|l|l|l|}
    \hline
    \textbf{Amount of instances} & \textbf{1} & \textbf{4} & \textbf{8} & \textbf{16} & \textbf{32} \\ \hline
    Shortest response time       & 29 ms      & 13 ms      & 19 ms      & 27 ms       & 55 ms       \\ \hline
    Longest response time        & 3451 ms    & 2609 ms    & 2509 ms    & 2387 ms     & 12182 ms    \\ \hline
    Average response time        & 771 ms     & 633 ms     & 593 ms     & 582 ms      & 657 ms      \\ \hline
    Failed assertions \%         & 20\%       & 9\%        & 9\%        & 6\%         & 8\%         \\ \hline
    \end{tabular}
    \caption{Summary of load test results.}
    \label{table:load-test-table}
\end{table}

For all tests, we asserted that endpoints should respond within 800 ms, or it should be tagged as failed.
As seen on \autoref{table:load-test-table}, the amount of failed assertions decreased as the cluster size increased, up until 32 instances, where the response times started increasing.

We deem that, even though the amount of VUs is relatively low, the system should be able to sustain a significantly higher amount of real users, as they would most likely not be sending constant requests in the same manner that the VUs do.
In general, we estimate that the average user would send less than one request per second on average, meaning that it is less than 18 times less load than the VUs.
This mean that we, in theory, should be able to support at least 20*18=360 arriving users every 10th second with this current setup.
For a real deployment of the service, the clusters would probably be split between multiple servers for horizontal scaling, and more specialized hardware that allows for better vertical scaling.
